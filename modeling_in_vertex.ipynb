{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic on Vertex AI - MLOps intoductory lab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a>\n",
    "       <img src=\"https://s9i7q5a6.rocketcdn.me/solutions/wp-content/uploads/2022/10/Vertex-AI-Logo.webp\" style=\"max-width: 25%; height: auto;\">\n",
    "    </a>\n",
    "    <a>\n",
    "       <img src=\"https://cdn1.naekranie.pl/media/cache/amp/2023/12/Titanic_6576d7d208376.jpg\" style=\"max-width: 50%; height: auto;\">\n",
    "    </a>\n",
    "  </td> \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "The purpose of this notebook is to present capabilities of Vertex AI in ML models development, serving and monitoring. It covers following components:\n",
    "\n",
    "- Exploratory Data Analysis\n",
    "- Model Experimentation\n",
    "- Model Training in Vertex AI\n",
    "- Model Serving\n",
    "- Model Monitoring (including Tensorboard)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages\n",
    "Import the following packages required to execute this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow-io==0.31.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s0_ixxYUX6KR",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import random\n",
    "import string\n",
    "\n",
    "from google.cloud import bigquery\n",
    "from tensorflow.keras import regularizers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score\n",
    "from google.cloud import aiplatform as vertex_ai\n",
    "\n",
    "# Set the seed fixed to make the output deterministic\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Generate unique ID to help w/ unique naming of certain pieces\n",
    "ID = \"\".join(random.choices(string.ascii_lowercase + string.digits, k=5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup your environment\n",
    "\n",
    "Run the next cell to set your project ID and some of the other constants used in the lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = '...' #ToDo: add project ID\n",
    "REGION = 'us-central1'\n",
    "BUCKET_NAME = \"gs://...\" #ToDo: select existing GS destination\n",
    "EXPERIMENT_NAME = f\"titanic-experiments-{ID}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize clients\n",
    "\n",
    "Next you have to initialize the Vertex AI SDK and the Python BigQuery Client for your project, region and corresponding bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vertex_ai.init(\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION,\n",
    "    staging_bucket=BUCKET_NAME,\n",
    "    experiment=EXPERIMENT_NAME,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bq_client = bigquery.Client(project=PROJECT_ID, location=REGION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data import and EDA\n",
    "\n",
    "The data is imported from storage.googleapis.com - it's split on train and eval sets - we concatenate them as we require split on: train, eval and test sets later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "titanic_t = pd.read_csv('https://storage.googleapis.com/tf-datasets/titanic/train.csv')\n",
    "titanic_e = pd.read_csv('https://storage.googleapis.com/tf-datasets/titanic/eval.csv')\n",
    "union_data = pd.concat([titanic_t, titanic_e], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Summary of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "union_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Examine the label distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.title(\"Count of passengers by survived label ('survived' = 0 or 1)\")\n",
    "sns.countplot(x=\"survived\", data=union_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now let's take a look at continuous predictors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "var = list(union_data.columns)\n",
    "var.remove('survived')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "union_data[var].hist(figsize=(20, 10), grid=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now for categorical ones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "categorical_vars = ['sex', 'class', 'deck', 'embark_town', 'alone'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create an empty dictionary to store the cross-tabulations\n",
    "cross_tabs = {}\n",
    "\n",
    "# Generate cross-tabulations for each categorical variable\n",
    "for var in categorical_vars:\n",
    "    cross_tabs[var] = pd.crosstab(index=union_data[var], columns='count')\n",
    "\n",
    "# Print the cross-tabulations\n",
    "for var, table in cross_tabs.items():\n",
    "    print(f\"Cross-tabulation for {var}:\\n{table}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run correlation matrix, first we run one-hot encoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "union_d = pd.get_dummies(union_data, columns=['sex', 'class', 'deck', 'embark_town', 'alone'])\n",
    "union_d = union_d * 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "variables = list(union_d.columns)\n",
    "variables.remove('survived')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 5))\n",
    "sns.heatmap(union_d[variables].corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering\n",
    "\n",
    "There is a possibility to run feature engineering using BigQuery - let's export the data first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dataset schema\n",
    "dataset_id = f\"{PROJECT_ID}.titanic_data\"\n",
    "\n",
    "dataset = bigquery.Dataset(dataset_id)\n",
    "dataset.location = \"us-central1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#ToDo: uncomment\n",
    "# create the dataset\n",
    "# dataset = bq_client.create_dataset(dataset, timeout=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# export data table\n",
    "table_id = f\"{PROJECT_ID}.titanic_data.titanic\"\n",
    "\n",
    "job_config = bigquery.LoadJobConfig(\n",
    "    autodetect=True,  \n",
    "    write_disposition=\"WRITE_TRUNCATE\"\n",
    ")\n",
    "\n",
    "union_d['age'] = union_d['age'].astype(int)\n",
    "job = bq_client.load_table_from_dataframe(\n",
    "    union_d, table_id, job_config=job_config\n",
    ")\n",
    "\n",
    "job.result() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can write any SQL statements you want - including creating new variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bigquery\n",
    "\n",
    "SELECT\n",
    "  AVG(titanic.age) AS avg_age,\n",
    "  titanic.embark_town_Cherbourg,\n",
    "  titanic.embark_town_Queenstown,\n",
    "  titanic.embark_town_Southampton,\n",
    "  titanic.embark_town_unknown\n",
    "FROM\n",
    "  `... .titanic_data.titanic` AS titanic #ToDo: put project ID in front of\n",
    "GROUP BY\n",
    "  2,\n",
    "  3,\n",
    "  4,\n",
    "  5;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimenting\n",
    "\n",
    "Before training the model, you can set some hyperparameters to help us improve the model's performance. We advise you to use Vertex AI Vizier, which automates the optimization of hyperparameters, to help with hyperparameter tuning. However, in this notebook, we specify these hyperparameters manually and randomly for the sake of simplicity and expedience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# data split\n",
    "train, test = train_test_split(union_d, test_size=0.2)\n",
    "train, val = train_test_split(train, test_size=0.25)\n",
    "print(len(train), 'train examples')\n",
    "print(len(val), 'validation examples')\n",
    "print(len(test), 'test examples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Monitoring with tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(len(variables),)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# a utility method to create a tf.data dataset from a Pandas Dataframe\n",
    "def df_to_dataset(dataframe, shuffle=True, batch_size=32):\n",
    "  dataframe = dataframe.copy()\n",
    "  labels = dataframe.pop('survived')\n",
    "  # Convert features to a numpy array with the correct shape\n",
    "  features = dataframe.values  \n",
    "  ds = tf.data.Dataset.from_tensor_slices((features, labels))\n",
    "  if shuffle:\n",
    "    ds = ds.shuffle(buffer_size=len(dataframe))\n",
    "  ds = ds.batch(batch_size)\n",
    "  return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 32 \n",
    "train_dataset = df_to_dataset(train, batch_size=batch_size)\n",
    "val_dataset = df_to_dataset(val, shuffle=False, batch_size=batch_size)\n",
    "test_dataset = df_to_dataset(test, shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear any logs from previous runs\n",
    "!rm -rf ./logs/ \n",
    "\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "history_model = model.fit(train_dataset, \n",
    "                                      validation_data=val_dataset,\n",
    "                                      epochs=50,\n",
    "                                      callbacks=[tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs/fit --port 6005"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Registering model experiments in Vertex AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "parameters = [\n",
    "    {\"nneur1\": 64, \"dropout\": 0.5, \"nneur2\": 30, \"l2reg\": 0.01},\n",
    "    {\"nneur1\": 64, \"dropout\": 0.6, \"nneur2\": 20, \"l2reg\": 0.10},\n",
    "    {\"nneur1\": 64, \"dropout\": 0.7, \"nneur2\": 10, \"l2reg\": 0.01},\n",
    "    {\"nneur1\": 32, \"dropout\": 0.5, \"nneur2\": 30, \"l2reg\": 0.01},\n",
    "    {\"nneur1\": 32, \"dropout\": 0.6, \"nneur2\": 20, \"l2reg\": 0.10},\n",
    "    {\"nneur1\": 32, \"dropout\": 0.7, \"nneur2\": 10, \"l2reg\": 0.01},\n",
    "    {\"nneur1\": 16, \"dropout\": 0.5, \"nneur2\": 30, \"l2reg\": 0.05},\n",
    "    {\"nneur1\": 16, \"dropout\": 0.6, \"nneur2\": 20, \"l2reg\": 0.10},\n",
    "    {\"nneur1\": 16, \"dropout\": 0.7, \"nneur2\": 10, \"l2reg\": 0.01},\n",
    "]\n",
    "\n",
    "models = {}\n",
    "for i, params in enumerate(parameters):\n",
    "    run_name = f\"titanic-model-{ID}-{i}\"\n",
    "    print(run_name)\n",
    "    vertex_ai.start_run(run=run_name)\n",
    "    vertex_ai.log_params(params)\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=(len(variables),)),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    model.fit(train_dataset,validation_data=val_dataset,epochs=50)\n",
    "    models[run_name] = model\n",
    "    \n",
    "    y_pred = model.predict(test_dataset)\n",
    "    y_pred = (y_pred >= 0.5).astype(int) \n",
    "    y_pred_series = pd.Series(y_pred.flatten()) \n",
    "    \n",
    "    y_pred = model.predict(test_dataset)\n",
    "    acc_score = accuracy_score(test['survived'], y_pred_series)\n",
    "    val_f1_score = f1_score(test['survived'], y_pred_series, average=\"weighted\")\n",
    "    vertex_ai.log_metrics({\"acc_score\": acc_score, \"f1score\": val_f1_score})\n",
    "    vertex_ai.end_run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a ML model using Vertex AI custom training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building a Vertex AI dataset\n",
    "\n",
    "In this section,we will create a managed Vertex AI dataset. Vertex AI datasets can be used to train AutoML models or custom-trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create Vertex AI managed dataset\n",
    "dataset = vertex_ai.TabularDataset.create(\n",
    "    display_name=\"titanic-dataset\",\n",
    "    bq_source=f\"bq://{table_id}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing model procedure for coming container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a folder for all container-related files\n",
    "!mkdir -p -m 777 build_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile build_training/train_model.py\n",
    "\n",
    "# Libraries --------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "import datetime\n",
    "import argparse\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.framework import dtypes\n",
    "from tensorflow_io.bigquery import BigQueryClient\n",
    "from tensorflow_io.bigquery import BigQueryReadSession\n",
    "\n",
    "from tensorflow.keras import regularizers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, f1_score, accuracy_score\n",
    "\n",
    "# Variables --------------------------------------------------------------------------------------------------------------------------\n",
    "\n",
    "def get_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # Data files arguments\n",
    "    parser.add_argument(\"--ID\", dest=\"ID\", type=str,\n",
    "                        required=True, help=\"generated random ID\")\n",
    "    \n",
    "    return parser.parse_args()\n",
    "\n",
    "def gcs_path_to_local_path(old_path):\n",
    "    new_path = old_path.replace(\"gs://\", \"/gcs/\")\n",
    "    return new_path\n",
    "\n",
    "# Read environmental variables\n",
    "args = get_args()\n",
    "ID = args.ID\n",
    "\n",
    "TRAINING_DATA_PATH = os.environ[\"AIP_TRAINING_DATA_URI\"]\n",
    "TEST_DATA_PATH = os.environ[\"AIP_TEST_DATA_URI\"]\n",
    "\n",
    "MODEL_DIR = gcs_path_to_local_path(os.environ[\"AIP_MODEL_DIR\"])\n",
    "MODEL_PATH = MODEL_DIR\n",
    "\n",
    "def main():\n",
    "\n",
    "    def features_and_labels(features):\n",
    "      label = features.pop('survived') # this is what we will train for\n",
    "      return features, label\n",
    "\n",
    "    def read_dataset(client, batch_size=48):\n",
    "        GCP_PROJECT_ID='...'  #ToDo: change\n",
    "        COL_NAMES = ['age', 'sex_female', 'class_First', 'class_Second', 'alone_y', 'survived']\n",
    "        COL_TYPES = [dtypes.int64, dtypes.int64, dtypes.int64, dtypes.int64, dtypes.int64, dtypes.int64]\n",
    "        DATASET_GCP_PROJECT_ID = GCP_PROJECT_ID\n",
    "        DATASET_ID, TABLE_ID,  = 'titanic_data.titanic'.split('.')\n",
    "        bqsession = client.read_session(\n",
    "            \"projects/\" + GCP_PROJECT_ID,\n",
    "            DATASET_GCP_PROJECT_ID, TABLE_ID, DATASET_ID,\n",
    "            COL_NAMES, COL_TYPES,\n",
    "            requested_streams=2)\n",
    "        dataset = bqsession.parallel_read_rows()\n",
    "\n",
    "        def preprocess(features):\n",
    "            label = features.pop('survived')\n",
    "            label = tf.cast(label, tf.int64)\n",
    "\n",
    "            numerical_features = []\n",
    "\n",
    "            for feature_name, feature_value in features.items():\n",
    "                    numerical_features.append(feature_name)\n",
    "\n",
    "            # Cast to tf.float64\n",
    "            numerical_features = [tf.cast(features[name], tf.int64) for name in numerical_features]\n",
    "\n",
    "            # Stack the numerical features into a 1D tensor\n",
    "            concatenated_features = tf.stack(numerical_features, axis=0) \n",
    "\n",
    "            return concatenated_features, label  # No need for tf.expand_dims\n",
    "\n",
    "        transformed_ds = dataset.map(preprocess)\n",
    "        return transformed_ds    \n",
    "\n",
    "    client = BigQueryClient()\n",
    "\n",
    "    newset = read_dataset(client, 2).batch(21)\n",
    "    \n",
    "    model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(5,)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
    "    tf.keras.layers.BatchNormalization(),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    model.fit(newset, validation_data=newset, epochs=50)\n",
    "\n",
    "    model.save(MODEL_PATH)\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building a Docker container"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "IMAGE_REPOSITORY = f\"titanic-repo-{ID}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "IMAGE_NAME = f\"classification-{ID}\"\n",
    "IMAGE_TAG = \"v1\"\n",
    "IMAGE_URI = f\"us-central1-docker.pkg.dev/{PROJECT_ID}/{IMAGE_REPOSITORY}/{IMAGE_NAME}:{IMAGE_TAG}\"\n",
    "TRAIN_COMPUTE = \"e2-standard-4\"\n",
    "DEPLOY_COMPUTE = \"n1-standard-4\"\n",
    "\n",
    "TRAIN_JOB_NAME = f\"titanic-train-{ID}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create image repository\n",
    "!gcloud artifacts repositories create $IMAGE_REPOSITORY      --repository-format=docker      --location=us-central1      --description=\"Titanic Docker Image repository\"\n",
    "\n",
    "# List repositories under the project\n",
    "!gcloud artifacts repositories list\n",
    "\n",
    "# Get info on the repository\n",
    "!gcloud artifacts repositories describe $IMAGE_REPOSITORY --location=us-central1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!gcloud auth configure-docker us-central1-docker.pkg.dev -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile build_training/Dockerfile\n",
    "# Specifies base image and tag\n",
    "FROM python:3.7\n",
    "WORKDIR /root\n",
    "\n",
    "# Installs additional packages\n",
    "RUN pip install tensorflow-io==0.31.0 tensorflow==2.11.0 gcsfs numpy pandas scikit-learn dask distributed xgboost --upgrade\n",
    "\n",
    "# Copies the trainer code to the docker image.\n",
    "COPY ./train_model.py /root/train_model.py\n",
    "\n",
    "# Sets up the entry point to invoke the trainer.\n",
    "ENTRYPOINT [\"python3\", \"train_model.py\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!docker build -t $IMAGE_URI ./build_training/\n",
    "!docker push $IMAGE_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training model in Vertex AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MODEL_SERVING_IMAGE_URI = (\n",
    "    \"us-docker.pkg.dev/vertex-ai/prediction/tf2-cpu.2-11:latest\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "job = vertex_ai.CustomContainerTrainingJob(\n",
    "    display_name=TRAIN_JOB_NAME,\n",
    "    container_uri=IMAGE_URI,\n",
    "    model_serving_container_image_uri=MODEL_SERVING_IMAGE_URI,\n",
    ")\n",
    "\n",
    "CMDARGS = [\n",
    "    f\"--ID={ID}\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_vertex = job.run(\n",
    "    dataset=dataset,\n",
    "    model_display_name= f\"titanic-vertex-{ID}\",\n",
    "    args=CMDARGS,\n",
    "    replica_count=1,\n",
    "    machine_type=TRAIN_COMPUTE,\n",
    "    accelerator_count=0,\n",
    "    bigquery_destination=f\"bq://{PROJECT_ID}\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deploying model to endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ENDPOINT_NAME = f\"titanic-{ID}-endpoint\"\n",
    "\n",
    "endpoint = model_vertex.deploy(\n",
    "    deployed_model_display_name=ENDPOINT_NAME,\n",
    "    traffic_split={\"0\": 100},\n",
    "    machine_type=\"n1-standard-4\",\n",
    "    accelerator_count=0,\n",
    "    min_replica_count=1,\n",
    "    max_replica_count=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting model monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling rate (optional, default=.8)\n",
    "LOG_SAMPLE_RATE = 1  # @param {type:\"number\"}\n",
    "\n",
    "# Monitoring Interval in hours (optional, default=1).\n",
    "MONITOR_INTERVAL = 1  # @param {type:\"number\"}\n",
    "\n",
    "# Skew and drift thresholds.\n",
    "DEFAULT_THRESHOLD = 0.003\n",
    "\n",
    "SKEW_THRESHOLDS = {\n",
    "    \"age\": DEFAULT_THRESHOLD,\n",
    "    \"alone_y\": DEFAULT_THRESHOLD,\n",
    "}\n",
    "DRIFT_THRESHOLDS = {\n",
    "    \"age\": DEFAULT_THRESHOLD,\n",
    "    \"alone_y\": DEFAULT_THRESHOLD,\n",
    "}\n",
    "ATTRIB_SKEW_THRESHOLDS = {\n",
    "    \"age\": DEFAULT_THRESHOLD,\n",
    "    \"alone_y\": DEFAULT_THRESHOLD,\n",
    "}\n",
    "ATTRIB_DRIFT_THRESHOLDS = {\n",
    "    \"age\": DEFAULT_THRESHOLD,\n",
    "    \"alone_y\": DEFAULT_THRESHOLD,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud.aiplatform import model_monitoring\n",
    "\n",
    "skew_config = model_monitoring.SkewDetectionConfig(\n",
    "    data_source=f\"bq://{table_id}\",\n",
    "    skew_thresholds=SKEW_THRESHOLDS,\n",
    "    attribute_skew_thresholds=ATTRIB_SKEW_THRESHOLDS,\n",
    "    target_field='survived',\n",
    ")\n",
    "\n",
    "drift_config = model_monitoring.DriftDetectionConfig(\n",
    "    drift_thresholds=DRIFT_THRESHOLDS,\n",
    "    attribute_drift_thresholds=ATTRIB_DRIFT_THRESHOLDS,\n",
    ")\n",
    "\n",
    "objective_config = model_monitoring.ObjectiveConfig(\n",
    "    skew_config, drift_config\n",
    ")\n",
    "\n",
    "# Create sampling configuration\n",
    "random_sampling = model_monitoring.RandomSampleConfig(sample_rate=LOG_SAMPLE_RATE)\n",
    "\n",
    "# Create schedule configuration\n",
    "schedule_config = model_monitoring.ScheduleConfig(monitor_interval=MONITOR_INTERVAL)\n",
    "\n",
    "# Create alerting configuration.\n",
    "emails = [\"...\"] #ToDo: fill\n",
    "alerting_config = model_monitoring.EmailAlertConfig(\n",
    "    user_emails=emails, enable_logging=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MONITORING_JOB_NAME = f\"titanic-monitoring-{ID}\"\n",
    "\n",
    "job_monitoring = vertex_ai.ModelDeploymentMonitoringJob.create(\n",
    "    display_name=MONITORING_JOB_NAME,\n",
    "    logging_sampling_strategy=random_sampling,\n",
    "    schedule_config=schedule_config,\n",
    "    alert_config=alerting_config,\n",
    "    objective_configs=objective_config,\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION,\n",
    "    endpoint=endpoint,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_requests = 1000\n",
    "\n",
    "while True:\n",
    "    print(\"Simulation started...\")\n",
    "    for idx in range(num_requests):\n",
    "        \n",
    "        request = [[0, 0, 0, 0, 0] for _ in range(5000)]\n",
    "\n",
    "        endpoint.predict(request)\n",
    "\n",
    "        if idx % 100 == 0:\n",
    "            print(f'{idx + 1} of {num_requests} prediction requests we`re invoked.')\n",
    "    print(\"Simulation finished.\")\n",
    "    \n",
    "    time.sleep(60*3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #close resources:\n",
    "\n",
    "# # [1] storage bucket\n",
    "# !gsutil -m rm -r gs://{BUCKET_NAME}/**\n",
    "# !gsutil rb gs://{BUCKET_NAME}\n",
    "\n",
    "# # [2] models and endpoints\n",
    "# def delete_all_models(PROJECT_ID, REGION):\n",
    "\n",
    "#     vertex_ai.init(project=PROJECT_ID, location=REGION)\n",
    "#     endpoints = vertex_ai.Endpoint.list()  # Get all endpoints\n",
    "\n",
    "#     for endpoint in endpoints:\n",
    "#         endpoint.undeploy_all()\n",
    "#         print(f\"Undeployed endpoints\")\n",
    "    \n",
    "#     for model in vertex_ai.Model.list():\n",
    "#         model.delete()\n",
    "#         print(f\"Deleted model: {model.name}\")\n",
    "\n",
    "# delete_all_models(PROJECT_ID, REGION)\n",
    "\n",
    "# # [3] experiments\n",
    "# experiments = vertex_ai.Experiment.list()\n",
    "# for experiment in experiments:\n",
    "#     experiment.delete(delete_backing_tensorboard_runs=True)\n",
    "\n",
    "# # [4] datasests\n",
    "# def delete_all_datasets():\n",
    "#     try:\n",
    "#         datasets = vertex_ai.TabularDataset.list() # or ImageDataset, TextDataset, etc.\n",
    "\n",
    "#         for dataset in datasets:\n",
    "#             dataset.delete()\n",
    "#             print(f\"Deleted dataset: {dataset.name}\")\n",
    "\n",
    "#         print(\"All datasets deleted successfully.\")\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error deleting datasets: {e}\")\n",
    "\n",
    "# delete_all_datasets()\n",
    "\n",
    "# # [5] featurestores\n",
    "\n",
    "# # [6] artifact registry\n",
    "# client = artifactregistry_v1.ArtifactRegistryClient()\n",
    "# repositories = client.list_repositories(parent=f\"projects/{PROJECT_ID}/locations/{REGION}\")\n",
    "# for repository in repositories:\n",
    "#     try:\n",
    "#         client.delete_repository(name=repository.name)\n",
    "#         print(f\"Deleted repository: {repository.name}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error deleting repository {repository.name}: {e}\")\n",
    "#     print(f\"Deleted featurestore: {featurestore.name}\")\n",
    "\n",
    "# # [7] metadata\n",
    "# # Initialize Metadata Service Client\n",
    "# metadata_client = MetadataServiceClient(client_options={\"api_endpoint\": f\"{REGION}-aiplatform.googleapis.com\"})\n",
    "\n",
    "# def delete_artifacts(metadata_store_id=\"default\"):  # Use 'default' for the default store\n",
    "#     parent = f\"projects/{PROJECT_ID}/locations/{REGION}/metadataStores/{metadata_store_id}\"\n",
    "#     artifacts = metadata_client.list_artifacts(parent=parent)\n",
    "\n",
    "#     for artifact in artifacts:\n",
    "#         metadata_client.delete_artifact(name=artifact.name)\n",
    "#         print(f\"Deleted artifact: {artifact.name}\")\n",
    "       \n",
    "# delete_artifacts()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "environment": {
   "kernel": "python3",
   "name": "tf2-cpu.2-11.m125",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/tf2-cpu.2-11:m125"
  },
  "kernelspec": {
   "display_name": "Python 3 (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
